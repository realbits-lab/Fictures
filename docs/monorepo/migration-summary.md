# ğŸ‰ Monorepo Setup Complete!

Your Fictures project is now ready for local AI model serving alongside Next.js.

## âœ… What's Been Created

### ğŸ“ New Structure
```
Fictures/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/          # Your Next.js app (ready to migrate)
â”‚   â””â”€â”€ ai-server/    # Python FastAPI server (ready to use)
â”œâ”€â”€ packages/
â”‚   â””â”€â”€ api-client/   # TypeScript client (ready to use)
â””â”€â”€ pnpm-workspace.yaml
```

### ğŸ Python AI Server (FastAPI)
- âœ… FastAPI application with CORS configured
- âœ… Text generation API endpoint (`/api/v1/text/generate`)
- âœ… Image generation API endpoint (`/api/v1/images/generate`)
- âœ… Pydantic schemas for type safety
- âœ… OpenAPI documentation (`/docs`)
- âœ… Health check endpoint (`/health`)

### ğŸ“¦ TypeScript API Client
- âœ… Type-safe client for AI server
- âœ… Auto-generation from OpenAPI schema
- âœ… Ready to use in Next.js

### ğŸ”§ Development Scripts
- âœ… `pnpm dev` - Run both servers
- âœ… `pnpm dev:web` - Next.js only
- âœ… `pnpm dev:ai` - AI server only
- âœ… All existing commands preserved

## ğŸš€ Next Steps

### 1. Run Migration Script (5 minutes)

```bash
# Make executable
chmod +x scripts/migrate-to-monorepo.sh

# Run migration
./scripts/migrate-to-monorepo.sh

# This moves your Next.js app to apps/web/
```

### 2. Install Dependencies (2 minutes)

```bash
# Install all workspace dependencies
pnpm install
```

### 3. Set Up Python Environment (3 minutes)

```bash
cd apps/ai-server

# Create virtual environment
python -m venv venv

# Activate it
source venv/bin/activate  # macOS/Linux

# Install dependencies
pip install -r requirements.txt
```

### 4. Test Everything (2 minutes)

```bash
# From monorepo root
pnpm dev

# You should see:
# - Next.js running on http://localhost:3000
# - FastAPI running on http://localhost:8000
```

### 5. Add Your AI Models

```bash
# Create models directory
mkdir -p apps/ai-server/models/text
mkdir -p apps/ai-server/models/images

# Download your models
# Examples:
# - Llama 3.2 for text generation
# - Stable Diffusion XL for images
# - FLUX.1 for fast image generation
```

### 6. Generate TypeScript Types

```bash
# Ensure AI server is running
pnpm dev:ai

# Generate types (in another terminal)
cd packages/api-client
pnpm generate
```

### 7. Use in Next.js

```typescript
// apps/web/src/app/api/ai/generate/route.ts
import { aiClient } from '@fictures/api-client';

export async function POST(request: Request) {
  const { prompt } = await request.json();

  const result = await aiClient.generateText({
    prompt,
    max_tokens: 1024,
  });

  return Response.json(result);
}
```

## ğŸ“š Documentation

### Essential Reading
1. **MONOREPO-SETUP.md** - Complete migration and setup guide
2. **docs/monorepo-architecture.md** - Architecture overview
3. **apps/ai-server/README.md** - AI server documentation
4. **packages/api-client/README.md** - API client usage

### Quick Commands

```bash
# Development
pnpm dev              # Run both servers
pnpm dev:web          # Next.js only
pnpm dev:ai           # AI server only

# Building
pnpm build            # Build Next.js
pnpm build:web        # Build Next.js (explicit)

# Database (unchanged)
pnpm db:generate      # Generate migrations
pnpm db:migrate       # Run migrations
pnpm db:studio        # Open Drizzle Studio

# Testing (unchanged)
pnpm test             # Run Jest tests
dotenv --file apps/web/.env.local run npx playwright test

# Type Generation
cd packages/api-client && pnpm generate
```

## ğŸ¯ What You Can Do Now

### 1. Text Generation
- Use local Llama, Mistral, or other LLMs
- No cloud API costs
- Full control over models

### 2. Image Generation
- Use Stable Diffusion XL, FLUX, or other models
- Generate story images locally
- Custom aspect ratios (your current 7:4 ratio supported)

### 3. Type Safety
- Python Pydantic models â†’ TypeScript types
- Catch API errors at compile time
- Auto-complete for all AI endpoints

### 4. Cost Savings
- Replace Google Gemini API calls with local models
- Pay once for GPU hardware
- Unlimited generations

## ğŸ”¥ Key Features

âœ… **Monorepo with pnpm workspaces**
âœ… **Python FastAPI with automatic API docs**
âœ… **TypeScript client with type generation**
âœ… **CORS configured for Next.js**
âœ… **Hot reload for both servers**
âœ… **Independent deployment ready**
âœ… **Existing Next.js app unchanged**
âœ… **All existing commands work**

## ğŸ› Troubleshooting

### Port already in use?
```bash
# Kill processes
lsof -ti:3000 | xargs kill -9  # Next.js
lsof -ti:8000 | xargs kill -9  # FastAPI
```

### Python issues?
```bash
# Recreate virtual environment
cd apps/ai-server
rm -rf venv
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### pnpm issues?
```bash
# Clear and reinstall
pnpm store prune
rm -rf node_modules pnpm-lock.yaml
pnpm install
```

## ğŸ“– Example: Adding Llama 3.2

```bash
# 1. Download model (example using Ollama)
ollama pull llama3.2:3b

# 2. Or download from Hugging Face
cd apps/ai-server/models/text
# Use huggingface-cli or manual download

# 3. Implement in apps/ai-server/src/services/text_service.py
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("path/to/llama-3.2")
tokenizer = AutoTokenizer.from_pretrained("path/to/llama-3.2")

# 4. Update apps/ai-server/src/routes/text_generation.py
# Replace placeholder with actual model inference

# 5. Test
curl -X POST http://localhost:8000/api/v1/text/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a story", "max_tokens": 512}'
```

## ğŸ¨ Example: Adding Stable Diffusion

```bash
# 1. Download model
cd apps/ai-server/models/images
# Download Stable Diffusion XL weights

# 2. Implement in apps/ai-server/src/services/image_service.py
from diffusers import StableDiffusionXLPipeline
import torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16
)
pipe.to("cuda")

# 3. Update apps/ai-server/src/routes/image_generation.py
# Replace placeholder with actual image generation

# 4. Test
curl -X POST http://localhost:8000/api/v1/images/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "A serene forest", "width": 1344, "height": 768}'
```

## ğŸš€ Deployment

### Next.js â†’ Vercel
```bash
cd apps/web
vercel deploy
```

### Python AI Server â†’ Railway
```bash
cd apps/ai-server
railway up
```

Or use Fly.io, Modal Labs, or any GPU cloud provider.

## âœ¨ You're All Set!

The monorepo structure is complete. All that's left is:

1. âœ… Run migration: `./scripts/migrate-to-monorepo.sh`
2. âœ… Install deps: `pnpm install`
3. âœ… Setup Python: `cd apps/ai-server && python -m venv venv && source venv/bin/activate && pip install -r requirements.txt`
4. âœ… Test: `pnpm dev`
5. ğŸ¨ Add your AI models!

**Questions?** Check the documentation files listed above.

---

**Happy building! ğŸ‰**
