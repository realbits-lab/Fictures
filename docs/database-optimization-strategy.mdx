---
title: "Database Optimization Strategy - PostgreSQL (Neon) & Redis"
---

# Database Optimization Strategy - PostgreSQL (Neon) & Redis

**Date:** October 25, 2025
**Status:** ✅ Phase 1 COMPLETED - Critical optimizations applied
**Goal:** Optimize database queries and indexing for maximum performance

---

## Current Performance Analysis

### Identified Bottlenecks

1. **N+1 Query Problem** (CRITICAL)
   - Location: `src/lib/db/relationships.ts:341-347`
   - Issue: Looping over chapters and querying scenes separately
   - Impact: 10 chapters = 11 database queries (1 for chapters + 10 for scenes)

2. **Missing Database Indexes** (HIGH PRIORITY)
   - No indexes on frequently queried columns
   - Impact: Full table scans on large tables

3. **Suboptimal Cache Key Structure**
   - Current: Simple string keys
   - Opportunity: Use Redis sorted sets for ordered data

---

## PostgreSQL (Neon) Optimizations

### 1. Database Indexes Strategy

#### Priority 1: Foreign Key Indexes (CRITICAL)

**Problem:** Foreign key columns are frequently used in WHERE clauses and JOINs but have no indexes.

**Solution:**
```sql
-- Stories table
CREATE INDEX idx_stories_author_id ON stories(author_id);
CREATE INDEX idx_stories_status ON stories(status);
CREATE INDEX idx_stories_status_created ON stories(status, created_at DESC);
CREATE INDEX idx_stories_view_count ON stories(view_count DESC); -- For popular stories

-- Chapters table
CREATE INDEX idx_chapters_story_id ON chapters(story_id);
CREATE INDEX idx_chapters_part_id ON chapters(part_id);
CREATE INDEX idx_chapters_story_order ON chapters(story_id, order_index);
CREATE INDEX idx_chapters_status ON chapters(status);

-- Parts table
CREATE INDEX idx_parts_story_id ON parts(story_id);
CREATE INDEX idx_parts_story_order ON parts(story_id, order_index);

-- Scenes table
CREATE INDEX idx_scenes_chapter_id ON scenes(chapter_id);
CREATE INDEX idx_scenes_chapter_order ON scenes(chapter_id, order_index);
CREATE INDEX idx_scenes_status ON scenes(status);

-- Characters table
CREATE INDEX idx_characters_story_id ON characters(story_id);

-- AI Interactions table
CREATE INDEX idx_ai_interactions_user_id ON ai_interactions(user_id);
CREATE INDEX idx_ai_interactions_created ON ai_interactions(created_at DESC);
```

**Expected Impact:**
- Query performance: **50-80% faster** for filtered queries
- JOIN operations: **70-90% faster**
- Popular story queries: **Instant** with view_count index

#### Priority 2: Composite Indexes (HIGH IMPACT)

**Problem:** Queries often filter by multiple columns (e.g., status + orderIndex).

**Solution:**
```sql
-- Composite indexes for common query patterns
CREATE INDEX idx_stories_author_status ON stories(author_id, status);
CREATE INDEX idx_chapters_story_status_order ON chapters(story_id, status, order_index);
CREATE INDEX idx_scenes_chapter_status_order ON scenes(chapter_id, status, order_index);
```

**Expected Impact:**
- Multi-column filters: **60-90% faster**
- Sorted queries: No additional sorting needed

#### Priority 3: Text Search Indexes (OPTIONAL)

**Problem:** Searching story titles/content is slow without full-text indexes.

**Solution:**
```sql
-- Full-text search on stories
CREATE INDEX idx_stories_title_search ON stories USING GIN (to_tsvector('english', title));
CREATE INDEX idx_stories_description_search ON stories USING GIN (to_tsvector('english', description));

-- Full-text search on scenes (if needed)
CREATE INDEX idx_scenes_content_search ON scenes USING GIN (to_tsvector('english', content));
```

**Expected Impact:**
- Text search: **10-100x faster** depending on data size

---

### 2. Fix N+1 Query Problem

#### Current Implementation (PROBLEMATIC)

```typescript
// src/lib/db/relationships.ts:341-347
const scenesByChapter: Record<string, typeof allScenes> = {};
for (const chapterId of chapterIds) {
  scenesByChapter[chapterId] = await db.select()
    .from(scenes)
    .where(eq(scenes.chapterId, chapterId))
    .orderBy(asc(scenes.orderIndex));
}
```

**Problem:**
- If story has 10 chapters, this makes **10 separate database queries**
- Each query has network latency + query execution time
- Total time: ~10 * (network latency + query time) = 500-1000ms

#### Optimized Implementation (SOLUTION)

```typescript
// Fetch ALL scenes for ALL chapters in a SINGLE query
const allScenes = await db.select()
  .from(scenes)
  .where(inArray(scenes.chapterId, chapterIds))
  .orderBy(asc(scenes.chapterId), asc(scenes.orderIndex));

// Group scenes by chapter in memory (fast)
const scenesByChapter: Record<string, typeof allScenes> = {};
for (const scene of allScenes) {
  if (!scenesByChapter[scene.chapterId]) {
    scenesByChapter[scene.chapterId] = [];
  }
  scenesByChapter[scene.chapterId].push(scene);
}
```

**Expected Impact:**
- Reduces 10 queries to **1 query**
- Network latency: **90% reduction** (1 round trip instead of 10)
- Total query time: **500-1000ms → 50-100ms** (10x faster)

---

### 3. Query Optimization Best Practices

#### A. Use Prepared Statements

**Problem:** Query compilation happens on every request.

**Solution:**
```typescript
// Drizzle automatically uses prepared statements
// But we can optimize by caching query builders

// Bad: Creates new query builder each time
export function getStoryById(id: string) {
  return db.select().from(stories).where(eq(stories.id, id));
}

// Good: Reuse query builder
const storyByIdQuery = db.select().from(stories).where(eq(stories.id, sql.placeholder('id')));

export function getStoryById(id: string) {
  return storyByIdQuery.execute({ id });
}
```

#### B. Select Only Required Columns

**Problem:** Selecting all columns wastes bandwidth and memory.

**Solution:**
```typescript
// Bad: Select all columns
const stories = await db.select().from(stories);

// Good: Select only needed columns
const stories = await db.select({
  id: stories.id,
  title: stories.title,
  status: stories.status,
  authorId: stories.authorId,
}).from(stories);

// Even better: Create reusable column sets
const storyListColumns = {
  id: stories.id,
  title: stories.title,
  status: stories.status,
  authorId: stories.authorId,
  viewCount: stories.viewCount,
  createdAt: stories.createdAt,
};

const stories = await db.select(storyListColumns).from(stories);
```

**Expected Impact:**
- Bandwidth: **30-70% reduction**
- Memory usage: **40-60% reduction**
- Parse time: **20-40% faster**

#### C. Optimize JOIN Strategy

**Problem:** Multiple LEFT JOINs can be slow.

**Solution:**
```typescript
// Consider separate queries + in-memory join for better performance
// Especially when one table is much larger than the other

// Option 1: Single query with JOIN (good for small datasets)
const result = await db.select()
  .from(stories)
  .leftJoin(chapters, eq(chapters.storyId, stories.id))
  .leftJoin(scenes, eq(scenes.chapterId, chapters.id))
  .where(eq(stories.id, storyId));

// Option 2: Separate queries + in-memory join (good for large datasets)
const story = await db.select().from(stories).where(eq(stories.id, storyId));
const chapters = await db.select().from(chapters).where(eq(chapters.storyId, storyId));
const scenes = await db.select().from(scenes).where(inArray(scenes.chapterId, chapterIds));

// Assemble in memory (fast)
const result = assembleStoryStructure(story, chapters, scenes);
```

---

## Redis Optimizations

### 1. Use Redis Sorted Sets for Ordered Data

**Problem:** Currently storing entire objects as JSON strings.

**Current Implementation:**
```typescript
// Store entire scene as JSON
await redis.set(`scene:${sceneId}:public`, JSON.stringify(scene), { ex: 3600 });
```

**Optimized Implementation:**
```typescript
// Use sorted sets for ordered collections
await redis.zadd(
  `chapter:${chapterId}:scenes`,
  { score: scene.orderIndex, member: scene.id }
);

// Store individual scenes as hashes (more efficient)
await redis.hset(`scene:${sceneId}`, {
  title: scene.title,
  content: scene.content,
  status: scene.status,
  wordCount: scene.wordCount.toString(),
  orderIndex: scene.orderIndex.toString(),
});

// Retrieve ordered scenes for a chapter
const sceneIds = await redis.zrange(`chapter:${chapterId}:scenes`, 0, -1);
const scenes = await Promise.all(
  sceneIds.map(id => redis.hgetall(`scene:${id}`))
);
```

**Expected Impact:**
- Memory: **20-40% reduction** (hashes vs JSON)
- Ordered retrieval: **O(log N)** instead of O(N)
- Partial updates: Update single field instead of entire object

### 2. Pipeline Operations for Batch Queries

**Problem:** Multiple Redis operations have individual network latency.

**Solution:**
```typescript
// Bad: Sequential operations
const story = await redis.get(`story:${storyId}`);
const chapters = await redis.get(`story:${storyId}:chapters`);
const scenes = await redis.get(`chapter:${chapterId}:scenes`);
// Total: 3 * network latency

// Good: Pipeline operations
const pipeline = redis.pipeline();
pipeline.get(`story:${storyId}`);
pipeline.get(`story:${storyId}:chapters`);
pipeline.get(`chapter:${chapterId}:scenes`);
const results = await pipeline.exec();
// Total: 1 * network latency
```

**Expected Impact:**
- Network latency: **70-90% reduction**
- Total time: **3-5x faster** for batch operations

### 3. Cache Key Structure Optimization

**Current Structure:**
```
story:{id}:public
story:{id}:structure:scenes:false:public
scene:{id}:public
```

**Optimized Structure:**
```
# Namespaced and hierarchical
fictures:story:{id}:public
fictures:story:{id}:chapters
fictures:chapter:{id}:scenes
fictures:scene:{id}:public

# Use sorted sets for collections
fictures:story:{id}:chapters (ZSET: score=orderIndex, member=chapterId)
fictures:chapter:{id}:scenes (ZSET: score=orderIndex, member=sceneId)
fictures:popular:stories (ZSET: score=viewCount, member=storyId)

# Use hashes for objects
fictures:story:{id} (HASH: title, status, authorId, etc.)
fictures:scene:{id} (HASH: title, content, status, etc.)
```

**Benefits:**
- Namespace collision prevention
- Efficient range queries with sorted sets
- Memory-efficient storage with hashes
- Better cache invalidation patterns

### 4. Cache Warming Strategy

**Problem:** First visit always hits database (cold cache).

**Solution:**
```typescript
// Background job to warm popular content
async function warmPopularStoriesCache() {
  // Get top 100 popular stories
  const popularStories = await db
    .select({ id: stories.id })
    .from(stories)
    .where(eq(stories.status, 'published'))
    .orderBy(desc(stories.viewCount))
    .limit(100);

  // Warm cache in batches
  const batchSize = 10;
  for (let i = 0; i < popularStories.length; i += batchSize) {
    const batch = popularStories.slice(i, i + batchSize);
    await Promise.all(
      batch.map(story => getStoryWithStructure(story.id, false))
    );
  }
}

// Run every 30 minutes
setInterval(warmPopularStoriesCache, 30 * 60 * 1000);
```

---

## Implementation Plan

### Phase 1: Critical Fixes ✅ COMPLETED

**Priority 1: Fix N+1 Query Problem** ✅
- ✅ Modified `src/lib/db/relationships.ts:getStoryWithStructure()`
- ✅ Replaced loop-based scene queries with single `inArray()` query
- ✅ Added performance logging

**Code Changes:**
```typescript
// BEFORE: N+1 query problem
for (const chapterId of chapterIds) {
  scenesByChapter[chapterId] = await db.select()
    .from(scenes)
    .where(eq(scenes.chapterId, chapterId))
    .orderBy(asc(scenes.orderIndex));
}

// AFTER: Single query
const allScenes = await db.select()
  .from(scenes)
  .where(inArray(scenes.chapterId, chapterIds))
  .orderBy(asc(scenes.chapterId), asc(scenes.orderIndex));

// Group in memory (fast)
const scenesByChapter = {};
for (const scene of allScenes) {
  if (!scenesByChapter[scene.chapterId]) {
    scenesByChapter[scene.chapterId] = [];
  }
  scenesByChapter[scene.chapterId].push(scene);
}
```

**Priority 2: Add Critical Indexes** ✅
- ✅ Created database migration `drizzle/0024_add_performance_indexes.sql`
- ✅ Applied 13 critical indexes to Neon database (128ms execution time)
- ✅ Created `scripts/apply-indexes.mjs` for migration execution

**Indexes Applied:**
- Stories: 5 indexes (author_id, status, view_count, composites)
- Chapters: 4 indexes (story_id, part_id, status, composites)
- Parts: 2 indexes (story_id, order_index)
- Scenes: 4 indexes (chapter_id, visibility, composites)
- Characters: 1 index (story_id)
- AI Interactions: 2 indexes (user_id, created_at)

**Actual Results:**
- Migration applied successfully: **128ms**
- Expected query performance improvement: **50-80% faster**
- Expected database load reduction: **60-80%**

### Phase 2: Redis Optimization (SHORT-TERM - 2 hours)

**Priority 1: Implement Pipeline Operations**
- Update `src/lib/cache/redis-cache.ts`
- Batch operations where possible

**Priority 2: Optimize Cache Key Structure**
- Migrate to namespaced keys
- Use sorted sets for ordered data

**Expected Results:**
- Cache operations: **2-3x faster**
- Memory usage: **20-30% reduction**

### Phase 3: Advanced Optimizations (LONG-TERM - 4 hours)

**Priority 1: Prepared Statements**
- Create reusable query builders
- Cache compiled queries

**Priority 2: Column Selection Optimization**
- Define column sets for different use cases
- Reduce data transfer

**Priority 3: Cache Warming**
- Implement background job
- Warm popular content proactively

**Expected Results:**
- Overall performance: **5-10x faster**
- Cache hit rate: **>98%**

---

## Testing Plan

### Test 1: N+1 Query Fix Verification

```bash
# Before optimization
dotenv --file .env.local run node scripts/test-query-performance.mjs
# Expected: 10 database queries, ~500-1000ms

# After optimization
dotenv --file .env.local run node scripts/test-query-performance.mjs
# Expected: 1 database query, ~50-100ms
```

### Test 2: Index Performance

```sql
-- Verify index usage
EXPLAIN ANALYZE
SELECT * FROM scenes
WHERE chapter_id = 'test-chapter-id'
ORDER BY order_index;

-- Should show "Index Scan" instead of "Seq Scan"
```

### Test 3: Redis Pipeline Performance

```bash
# Test batch operations
dotenv --file .env.local run node scripts/test-redis-pipeline.mjs
# Expected: 3-5x faster than sequential operations
```

---

## Performance Targets

| Optimization | Current | Target | Expected Improvement |
|--------------|---------|--------|---------------------|
| **N+1 Query Fix** | 500-1000ms | 50-100ms | **10x faster** |
| **Database Indexes** | 200-500ms | 50-100ms | **3-5x faster** |
| **Redis Pipeline** | 150ms | 50ms | **3x faster** |
| **Column Selection** | 100KB | 30KB | **70% reduction** |
| **Cache Warming** | 3.7s cold | 0.5s | **7x faster** |

**Overall Expected Improvement:**
- Cold cache: 3.7s → **<0.5s** (7x faster)
- Warm cache: 0.5s → **<0.1s** (5x faster)
- Database load: **60-80% reduction**

---

## Monitoring

### Key Metrics

1. **Database Query Count**
   ```typescript
   console.log('[DB] Query count:', queryCount);
   console.log('[DB] Total query time:', totalQueryTime);
   ```

2. **Index Usage**
   ```sql
   SELECT * FROM pg_stat_user_indexes
   WHERE schemaname = 'public';
   ```

3. **Redis Performance**
   ```bash
   redis-cli INFO stats
   redis-cli SLOWLOG GET 10
   ```

4. **Cache Hit Rate**
   ```typescript
   const hitRate = (hits / (hits + misses)) * 100;
   console.log('[Cache] Hit rate:', hitRate, '%');
   ```

---

## Files to Modify

1. **src/lib/db/relationships.ts** - Fix N+1 query
2. **drizzle/migrations/*** - Add database indexes
3. **src/lib/cache/redis-cache.ts** - Implement pipeline operations
4. **scripts/test-query-performance.mjs** - Create performance test
5. **scripts/warm-cache.mjs** - Create cache warming script

---

## Next Steps

1. ✅ Create this optimization strategy document
2. ⏳ Fix N+1 query problem (NEXT)
3. ⏳ Create database migration for indexes
4. ⏳ Implement Redis pipeline operations
5. ⏳ Create performance test scripts
6. ⏳ Test and verify improvements
7. ⏳ Update performance-optimization-summary.md with results
